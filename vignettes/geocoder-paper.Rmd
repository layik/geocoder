---
title: "geocoder: an R package to store and access geographic data in MongoDB"
author: 
  - "L Hama"
date: "`r Sys.Date()`"
output: rmarkdown::github_document
vignette: >
  %\VignetteIndexEntry{geocoder-paper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%"
)
```

## Introduction
Geocodes [@goldberg2007text] are spatial references which are used to identify points or boundaries in a geographic coordinate system. Finding geocodes is a cumbersome and repeated task for data scientists. In the case of the United Kingdom, despite the availability of "open geography portal" [@office2016open], data scientists need to locate the relevant source, which could be typically an ESRI shape or sometimes a comma separated values (CSV) file. In the case of those who use R as a data science tool, this also means processing these files repeatedly. Having these geocodes handy which can be easily accessed is what geospatial database management systems (DBMS) such as PostGIS and MongoDB should be doing. The idea of having an R package that can fascilitate this is the reason for a missing R package called `geocoder`. In the age of availability of wide range of SQL/NoSQL and other big data oriented DBMS'es, finding geocodes should be easier. 

## Motivation
The Turing Institute's [eAtlas](https://www.turing.ac.uk/research/research-projects/turing-geovisualization-engine) project [@tge] is focused on visualizing spatially referenced datasets. It would be much easier to get the geographic data from datasets using databases rather than data science workflows in R/Python. Whilst developing a web based geovisualization eAtlas and allowing users to "load" their datasets in different formats, it would be great to detect geocodes and find coordinate data (such as longitute and latitudes) "automatically". As it stands, there is no such open API (Application Programming Interface) to query for such geographic data and availability of such data locally (to the application) is becoming a need.

## Relevant technology
The standardisation of storage and access model of two-dimentional geographic references by both Open Geospatial Consortium (OGC) and International Organization for Standardisation (ISO) means programmers can define objects which represent these standards. This standard is known as [Simple Features](https://www.iso.org/standard/40114.html). In the case of R, the well known package called `sf` [@pebesma2018simple] defines the standard and provides tools to manage geographic data in R. MongoDB is one of the DBMS'es which also support this standard.

MongoDB comes with built-in geospatial indexing and querying [@ameri2014application] around GeoJSON [@butler2016geojson], which is another standard built on top of Simple Featurers. MongoDB accepts GeoJSON formatted coordinates, and database indices can be created which fascilitate fast spatial queries. As a data interchange format, GeoJSON is also widely supported by mapping libraries [@page2015visualising]. GeoJSON is JSON which is natively supported (read) by JavaScript and might be the reason for its wider popularity.


## UK geocodes case

UK Office of National Statistics Open Geography Portal contains various files which are used by data scientists to plot data on maps. Details of different layers etc.

### Round trip through MongoDB
To showcasee the capabilities of `geocoder` package, we can do a round trip data processing from a seudo-standard R spatial `sf` class back to itself through MongoDB's geospatial querying. That is:

* take a shapefile from UK's Open Geography Portal

* turn it into an `sf` object

* write it into a MongoDB collection and create a spatial index on it.

* reassemble the output from find queries back to an `sf` object.

One of the UK census boundaries is called Middle Superr Output Area (MSOA), 
which can be found 
[here](https://opendata.arcgis.com/datasets/f341dcfd94284d58aba0a84daf2199e9_0.zip).

```{r msoa, eval=FALSE}
msoa.folder = file.path(tempdir(), "msoa_folder")
msoa.zip = file.path(tempdir(), "msoa.zip")
if(!exists(msoa.file)) {
  download.file(
    paste0("https://opendata.arcgis.com/datasets/",
           "f341dcfd94284d58aba0a84daf2199e9_0.zip"),
    msoa.zip)
  unzip(msoa.zip, exdir = msoa.folder)
}
# read the shape file using `sf`
msoa.sf = sf::read_sf(
  list.files(msoa.folder, pattern = "England_and_Wales.shp"))
class(msoa.sf)
# substring(geojsonsf::sf_geojson(msoa.sf[1,]), 1, 350)
```

The data is ready in a clean `sf` object and we can convert each row into a ready to be used by `geocder` to import into Mongodb:

```{r eval=FALSE}
geocoder::gc_import_sf(msoa.sf, collection = "msoa")
```

We can now query the database with lists of MSOA codes and easily return GeoJSON formatted "features" ready to be reassembled into `sf`.

```{r, eval=FALSE}
r = gc_find(data.frame(properties.msoa01cd="E02000998"), collection = "msoa")
class(r)
# [1] "data.frame"
substr(r, 1, 200)
# [1] "Feature"
# [2] "list(objectid = 998, msoa01cd = \"E02000998\", ..."

```
### Find geocodes with spatial reference

```{r findMSOAcode, eval=FALSE}
# head(st_coordinates(st_geometry(msoa.sf[1,])))
# code for point -0.09640594 51.52283
qry <- '[
  {
    "$geoNear" : { 
      "near" : { "type" : "Point", "coordinates" : [ -0.09640594, 51.52283 ] },
      "distanceField" : "dist.calculated",
      "maxDistance" : 1,
      "spherical" : true
    }
  }
] '
r = msoa.collection$aggregate(pipeline = qry)
r$properties$msoa01cd
# "E02000001" "E02000576"
```
The returned values are two different geometries which both correctly include the poin in the query.

## Reproducible example

A reproducible (as the UK msoa example is too large) for this manuscrript would be a slice of Uber's Vancouver land area price.

```{r reproducible, out.width="50%", fig.align='center'}
# geojson file from Uber
library(sf)
v.path = file.path(tempdir(), "vancouver.json")
if(!exists(v.path)) {
  v.url = paste0("https://github.com/layik/geocoder/releases/",
                 "download/data/v10.geojson")
  download.file(v.url, v.path)
}

v = geojsonsf::geojson_sf(v.path)
vancouver = mongolite::mongo("vancouver")
# plot(v[8, ], axes = TRUE)
```

Let us retrieve the data and plot the land:
```{r reproducible2, out.width="50%", fig.align='center'}
vancouver$drop() # if exists
geocoder::gc_import_sf(v, collection = "vancouver")
r = vancouver$iterate(query = '{"properties.growth":0.2781}')
r.sf = geojsonsf::geojson_sf(r$json())
```

Visual check that both entries are the same:

```{r, echo=FALSE, fig.show = "hold", out.width = "48%"}
library(ggplot2)
ggplot() +
  geom_sf(data = v[8, ], colour = "red", fill = NA) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1))
ggplot() +
  geom_sf(data = r.sf, colour = "red", fill = NA) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1))
```

## R vs Mongodb
We can look at a use-casee where instead of searching through geocodes in memory we query a local database. For example, instead of loadinig one ore more files and converting them into `sf` or similar like objecsts in R, we search an spatial index in a MongoDB instance. Lets assume you have loaded the Vancouver real estate dataset into R and the same dataset has been imported using into MongoDB using `geocoder::gc_import_sf()` function. 

First we can make sure that a simple query returns the same number of rows:

```{r, eval=FALSE}
v$count('{"properties.growth": 0.2724}') == 
  length(which(sf$growth == 0.2724))
# [1] TRUE
```
It is also fair to assume that for a data scientist to make a spatiial query, we have to load the dataset into memory first. We know that in-memory databases are built to manage in-memory access to data, but that is not the focus at the moment. 

```{r inR, results='hide', warning=FALSE, message=FALSE}
url = paste0("https://github.com/uber-common/deck.gl-data/",
             "blob/master/examples/geojson/vancouver-blocks.json?raw=true")
v.path = file.path(tempdir(), "v.geojson")
if(!file.exists(v.path)) {
  download.file(url, destfile = v.path)
}
# sampling the data
sf = st_read(v.path)
cent = st_centroid(st_as_sfc(st_bbox(sf)))
cent = st_transform(cent, 3488)
circle = st_buffer(cent, dist = 3000)
circle = st_transform(circle, 4326)

# import sf into Mongodb
v = mongolite::mongo(collection = "test_geocoder")
v$drop() # just in case
geocoder::gc_import_sf(sf, collection = "test_geocoder")

# scoping
sffind = NULL
vfind = NULL
# load and search
sf_search = function() {
  sf = st_read(v.path)
  # in memory search (R)
  sffind <<- st_intersection(sf, st_as_sf(circle))
}

# Mongo search
mongo_search = function() {
  vfind <<- v$iterate(paste0('{
     "geometry": { 
        "$geoIntersects": {
            "$geometry": ', 
             geojsonsf::sf_geojson(st_as_sf(circle), atomise = TRUE )
          ,'
        }
      }
     }'))
}
mb = microbenchmark::microbenchmark(
  sf_search(),
  mongo_search(),
  times = 1
)
```
## Conclusion
Looking at the results, which understandably varies considerably between the two methods because in the case of searching in memory, although it is faster, we need to load thole data (1.5mb) into memory first. To paint the whole picture, under the same circumstances, even if the whole data is queried, searching through MongoDB would only be three times slower.
```{r show, echo=FALSE}
mb
```

Visual check:

```{r, echo=FALSE, fig.show = "hold", out.width = "48%"}
plot(sffind["valuePerSqm"], main="sf::st_intersection")
vfind = geojsonsf::geojson_sf(vfind$json())
plot(vfind["valuePerSqm"], main="Mongo geoIntersects")
```


## References


